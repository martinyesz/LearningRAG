# llama_index_demo.py
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import Settings

# è®¾ç½®åµŒå…¥æ¨¡å‹ï¼ˆæœ¬åœ°å…è´¹æ¨¡å‹ï¼‰
Settings.embed_model = HuggingFaceEmbedding(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# ç¦ç”¨ LLMï¼ˆé¿å…ä¾èµ– OpenAIï¼‰
Settings.llm = None

# 1. åŠ è½½æ•°æ®
print("ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®...")
from llama_index.core import Document
import json
import os

documents = []
for filename in os.listdir("./data"):
    if filename.endswith(".json"):
        with open(os.path.join("./data", filename), "r", encoding="utf-8") as f:
            data = json.load(f)
            documents.append(Document(text=data["content"], metadata=data["metadata"]))

# 2. æ„å»ºå‘é‡ç´¢å¼•
print("ğŸ—ï¸  æ­£åœ¨æ„å»ºå‘é‡ç´¢å¼•...")
index = VectorStoreIndex.from_documents(documents)

# 3. åˆ›å»ºæŸ¥è¯¢å¼•æ“
print("ğŸ” åˆ›å»ºæŸ¥è¯¢å¼•æ“...")
query_engine = index.as_query_engine(similarity_top_k=2)  # è¿”å›æœ€ç›¸ä¼¼çš„2ä¸ªç»“æœ

# 4. æ‰§è¡Œç›¸ä¼¼æ€§æœç´¢
queries = [
    "å“ªç§æ°´æœå¯Œå«ç»´ç”Ÿç´ Cï¼Ÿ",
    "è¿åŠ¨å‘˜é€‚åˆåƒä»€ä¹ˆæ°´æœè¡¥å……èƒ½é‡ï¼Ÿ",
    "å¯¹å¿ƒè„æœ‰ç›Šçš„æ°´æœæ˜¯ä»€ä¹ˆï¼Ÿ"
]

for i, query in enumerate(queries, 1):
    print(f"\n--- æŸ¥è¯¢ {i}: {query} ---")
    response = query_engine.query(query)
    print("ğŸ’¬ å›ç­”:", str(response))
    # æ‰“å°å‚è€ƒæ¥æºï¼ˆå¯é€‰ï¼‰
    print("ğŸ“„ å‚è€ƒæ¥æº:")
    for node in response.source_nodes:
        print(f"  - {node.node.text[:100]}...")

print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")